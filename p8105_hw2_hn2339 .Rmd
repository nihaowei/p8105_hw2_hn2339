---
title: "p8105_hw2_hn2339"
author: "Haowei Ni"
date: "2018/9/30"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(dplyr)
```

#Question 1 
```{r}
NYC_Subway <- read_csv("data /NYC_Subway.csv") %>%
  # clean the name to lower case 
  janitor::clean_names() %>%
  # remove the unwante columns 
  select(-division, -exit_only, -staffing, -staff_hours, -free_crossover, -north_south_street, -east_west_street, -corner, -station_location, -entrance_location, -entrance_longitude, -entrance_latitude) %>%
  # convert the entry variable from "YES NO" to "TRUE FALSE"
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE)) 
```
There are totally 20 variables in this data set. Retain line, statation name, station longitude/latitude, route1~11, entrance type, entry, vending and ADA compliance. Step 1 is change the capital letter to lower case. Step 2 is remove all the unwanted columns. Step 3 is change the entry variable (YES or NO) to logical variable (TRUE or FLASE). The data is not tidy yet. 

```{r}
 #find the distinct station, . keep_all = TRUE keep all other data 
distinct_station = distinct(NYC_Subway, station_name, line, .keep_all = TRUE) 
   dim(distinct_station)
# find out ADA compliant and the proportion 
ADA_compliant = filter(distinct_station, ada == TRUE)
   dim(ADA_compliant)
Without_vending = filter(NYC_Subway, vending == "NO", entry == TRUE)
   dim(Without_vending)
Without_vending_no = filter(NYC_Subway, vending == "NO")
   dim(Without_vending_no)
```
There are totally 465 distinct stations both by name and by line. 
There are totally 84 stations are ADA compliant. 
The proportion is 69/183 = 37.7%

```{r}
# use gather function to make route number and name to be distinct variables
reformat_data = gather(NYC_Subway, key = route_number, value =route_name, route1:route11 )
# use distinct to find unique stations serve A train 
   distinct_AA = distinct(reformat_data, station_name, line, route_name, .keep_all = TRUE) 
   distinct_A = filter(distinct_AA, route_name == "A")
   dim(distinct_A) 
# filter out the non-ADA 
ADA = filter(distinct_A, ada == TRUE)
   dim(ADA) 
```
There are totally 60 stations. 
There are 17 ADA compliant.

#Question 2 
```{r}
library(readxl)
Mr_TrashWheel = read_excel("data /HealthyHarborWaterWheelTotals2018-7-28.xlsx", range = "A2:N338") %>%
  janitor::clean_names() %>%
  filter(!is.na(dumpster)) %>%
# round the sport_balls to nearest integer and convert 
  mutate(sports_balls = as.integer(round(sports_balls, 0)))
  Mr_2016 = filter(Mr_TrashWheel, year == 2016)
```

```{r}
library(readxl)
# use sheet command to specify the exact data set, cell color? 
Precipation_2016 = read_excel("data /HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = 5, range = "A2:B15") %>%
  janitor::clean_names() %>%
  filter(!is.na(total)) %>%
  mutate(Year = "2016")
Precipation_2017 = read_excel("data /HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = 4, range = "A2:B15") %>%
  janitor::clean_names() %>%
  filter(!is.na(total)) %>%
  mutate(Year = "2017")
# bind the 2016 and 2017 data set together 
Precipation = bind_rows(Precipation_2016, Precipation_2017) %>%
  mutate(month = month.name[month])
  P2017 = filter(Precipation, Year == 2017) 
  prcp2017 = filter(P2017, !is.na(month))
```
The dimension of the Mr_TrashWheel is `r dim(Mr_TrashWheel)`. There are total 14 variables. Dumpster, volumes and weight, homes powered and other kinds of trashes are the key variables. 
The dimension of the Precipation is `r dim(Precipation)` There are total 3 variables. Month, total precipation and year. 
The median number of sports balls in a dumpster in 2016 is `r median(Mr_2016$sports_balls)`
The total precipation in 2017 is `r sum(prcp2017$total)`

#Question 3 

```{r}
# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets")
```

```{r}
library(p8105.datasets)
    data(brfss_smart2010) 
    BRFSS = janitor::clean_names(brfss_smart2010) %>%
    filter(topic == "Overall Health") %>%
# removed the unwanted columns 
    select(-c(class, topic, question, sample_size, confidence_limit_low:geo_location)) %>%
# spread the response column into value 
    spread(key = response, value = data_value) 
    Scatter_plot = filter(BRFSS, locationdesc == "NY - New York Country" | locationdesc == "NY - Queens Country")
    destination = distinct(BRFSS, locationdesc, .keep_all = TRUE) 
    state = distinct(BRFSS, locationabbr, .keep_all = TRUE)
    most = tail(names(sort(table(BRFSS$locationabbr))), 1)
    Excellent_2002 = filter(BRFSS, year == "2002") %>%
    #create a new variable in the last column to present the proportion of excellent and very good 
    mutate(proportion = (Excellent + `Very good` ) / (Excellent + Fair + Good + Poor + `Very good`))
#make the histogram 
    ggplot(Excellent_2002, aes(x = Excellent)) +
      geom_histogram(binwidth = 1)
#make the scatterplot 
    Scatter = filter(BRFSS, locationdesc == "NY - New York County" | locationdesc == "NY - Queens County")
    ggplot(Scatter, aes(x = year, y = Excellent)) + 
      geom_point(aes(color = locationdesc), alpha = 0.5)
```
The number of unique destination is `r nrow(destination)`. 
The number of unique state is `r nrow(state)`. So every state is represented. 
The most state which observed most is `r most`. 
The median of 2002 excellent is `r median(Excellent_2002$Excellent, na.rm = TRUE)`
From the histogram plot we can see that most excellent scores are between 20-30. There is one outlier which has excellent score over 40. The graph is right skewed, which means that the median score is less than the mean. 
From the scatterplot we can see that excellent score in NY Queens country is lower than NY New York country. All Queens country's scores are below 22.5, on the other hand, New country's scores are above 22.5. The scores are fluctuate between 2002 and 2010 with no increasing or decreasing trend. 

